{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNV3KEdvKmexDgSNCxEPcyg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parwinderau/DataspaceConnector/blob/main/JSONTokenizerBertModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLe9pOIJTuRv",
        "outputId": "acbd6d95-372c-441d-f77d-b7005585ff69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install tensorflow\n",
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class JsonHarmonizer:\n",
        "    def __init__(self, model_name='bert-base-uncased'):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_ids')\n",
        "        attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "        bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "        bert_output = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
        "        cls_token = bert_output[:, 0, :]  # Use the output for the [CLS] token\n",
        "\n",
        "        dense = tf.keras.layers.Dense(128, activation='relu')(cls_token)\n",
        "        output = tf.keras.layers.Dense(2, activation='softmax')(dense)  # Assuming binary classification for simplicity\n",
        "\n",
        "        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def preprocess(self, json_data):\n",
        "        def flatten_json(y):\n",
        "            out = {}\n",
        "\n",
        "            def flatten(x, name=''):\n",
        "                if type(x) is dict:\n",
        "                    for a in x:\n",
        "                        flatten(x[a], name + a + '_')\n",
        "                elif type(x) is list:\n",
        "                    i = 0\n",
        "                    for a in x:\n",
        "                        flatten(a, name + str(i) + '_')\n",
        "                        i += 1\n",
        "                else:\n",
        "                    out[name[:-1]] = x\n",
        "            flatten(y)\n",
        "            return out\n",
        "\n",
        "        flat_json = [flatten_json(item) for item in json_data]\n",
        "        text_data = [' '.join(f'{k} {v}' for k, v in item.items()) for item in flat_json]\n",
        "        return text_data\n",
        "\n",
        "    def tokenize(self, text_data):\n",
        "        encodings = self.tokenizer(text_data, truncation=True, padding=True, return_tensors='tf')\n",
        "        return encodings\n",
        "\n",
        "    def train(self, json_data, labels):\n",
        "        text_data = self.preprocess(json_data)\n",
        "        encodings = self.tokenize(text_data)\n",
        "\n",
        "        # Ensure all encodings and labels are tensors\n",
        "        input_ids = tf.convert_to_tensor(encodings['input_ids'])\n",
        "        attention_mask = tf.convert_to_tensor(encodings['attention_mask'])\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        # Split the data\n",
        "        X_train_ids, X_val_ids, X_train_mask, X_val_mask, y_train, y_val = train_test_split(\n",
        "            input_ids.numpy(), attention_mask.numpy(), labels, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Convert arrays back to tensors\n",
        "        X_train_ids = tf.convert_to_tensor(X_train_ids)\n",
        "        X_train_mask = tf.convert_to_tensor(X_train_mask)\n",
        "        X_val_ids = tf.convert_to_tensor(X_val_ids)\n",
        "        X_val_mask = tf.convert_to_tensor(X_val_mask)\n",
        "        y_train = tf.convert_to_tensor(y_train)\n",
        "        y_val = tf.convert_to_tensor(y_val)\n",
        "\n",
        "        # Debug: Print shapes of tensors\n",
        "        print(f'X_train_ids shape: {X_train_ids.shape}')\n",
        "        print(f'X_train_mask shape: {X_train_mask.shape}')\n",
        "        print(f'X_val_ids shape: {X_val_ids.shape}')\n",
        "        print(f'X_val_mask shape: {X_val_mask.shape}')\n",
        "        print(f'y_train shape: {y_train.shape}')\n",
        "        print(f'y_val shape: {y_val.shape}')\n",
        "\n",
        "        # Training the model\n",
        "        self.model.fit(\n",
        "            [X_train_ids, X_train_mask], y_train,\n",
        "            validation_data=([X_val_ids, X_val_mask], y_val),\n",
        "            epochs=3,\n",
        "            batch_size=16\n",
        "        )\n",
        "\n",
        "    def harmonize(self, json_input):\n",
        "        def flatten_json(y):\n",
        "            out = {}\n",
        "\n",
        "            def flatten(x, name=''):\n",
        "                if type(x) is dict:\n",
        "                    for a in x:\n",
        "                        flatten(x[a], name + a + '_')\n",
        "                elif type(x) is list:\n",
        "                    i = 0\n",
        "                    for a in x:\n",
        "                        flatten(a, name + str(i) + '_')\n",
        "                        i += 1\n",
        "                else:\n",
        "                    out[name[:-1]] = x\n",
        "            flatten(y)\n",
        "            return out\n",
        "\n",
        "        flat_input = flatten_json(json_input)\n",
        "        text_input = ' '.join(f'{k} {v}' for k, v in flat_input.items())\n",
        "        encodings = self.tokenize([text_input])\n",
        "        input_ids = tf.convert_to_tensor(encodings['input_ids'])\n",
        "        attention_mask = tf.convert_to_tensor(encodings['attention_mask'])\n",
        "\n",
        "        predictions = self.model.predict([input_ids, attention_mask])\n",
        "        predicted_label = np.argmax(predictions, axis=-1)\n",
        "\n",
        "        # Placeholder harmonization logic based on predicted label\n",
        "        harmonized_output = {\n",
        "            \"entity\": flat_input.get(\"factory_name\", flat_input.get(\"manufacturer_employee\", flat_input.get(\"maker_person\", \"\"))),\n",
        "            \"product\": flat_input.get(\"factory_product\", flat_input.get(\"manufacturer_item\", flat_input.get(\"maker_goods\", \"\")))\n",
        "        }\n",
        "        return harmonized_output\n",
        "\n",
        "# Example usage\n",
        "json_data = [\n",
        "    {'factory': {'name': 'Alice', 'product': 'car'}},\n",
        "    {'manufacturer': {'employee': 'Alice', 'item': 'car'}}\n",
        "]\n",
        "\n",
        "# Labels for training, assuming a simple binary classification task\n",
        "labels = [0, 1]\n",
        "\n",
        "harmonizer = JsonHarmonizer()\n",
        "harmonizer.train(json_data, labels)\n",
        "input_json = {'maker': {'person': 'Charlie', 'goods': 'computer'}}\n",
        "harmonized_output = harmonizer.harmonize(input_json)\n",
        "print(harmonized_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEcsDUThU5K0",
        "outputId": "348d28f2-b9f7-42e9-f77b-4ce6543515b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_ids shape: (1, 10)\n",
            "X_train_mask shape: (1, 10)\n",
            "X_val_ids shape: (1, 10)\n",
            "X_val_mask shape: (1, 10)\n",
            "y_train shape: (1,)\n",
            "y_val shape: (1,)\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 43s 43s/step - loss: 0.5551 - accuracy: 1.0000 - val_loss: 0.9213 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3353 - accuracy: 1.0000 - val_loss: 1.5027 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2282 - accuracy: 1.0000 - val_loss: 2.2152 - val_accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "{'entity': 'Charlie', 'product': 'computer'}\n"
          ]
        }
      ]
    }
  ]
}