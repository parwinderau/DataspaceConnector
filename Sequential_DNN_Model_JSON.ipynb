{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLFkunqnfkJmanU62TfOci",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parwinderau/DataspaceConnector/blob/main/Sequential_DNN_Model_JSON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PelJepbjwt_",
        "outputId": "8ec2f57d-9a2f-4a8b-bbb5-c38c996f8cdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 2.2928\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.0000e+00 - loss: 2.2910\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5000 - loss: 2.2720\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5000 - loss: 2.2228\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5000 - loss: 2.2421\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5000 - loss: 2.2153\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 2.1957\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5000 - loss: 2.2163\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: 2.1305\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5000 - loss: 2.1279\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "Predicted schema category: 7\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Step 1: Load and Flatten JSON Data\n",
        "def flatten_json(y):\n",
        "    out = {}\n",
        "    def flatten(x, name=''):\n",
        "        if type(x) is dict:\n",
        "            for a in x:\n",
        "                flatten(x[a], name + a + '_')\n",
        "        elif type(x) is list:\n",
        "            i = 0\n",
        "            for a in x:\n",
        "                flatten(a, name + str(i) + '_')\n",
        "                i += 1\n",
        "        else:\n",
        "            out[name[:-1]] = x\n",
        "    flatten(y)\n",
        "    return out\n",
        "\n",
        "# Example JSON Data\n",
        "json_data = '''\n",
        "[\n",
        "  {\n",
        "    \"device_id\": \"sensor1\",\n",
        "    \"timestamp\": \"2024-08-13T10:00:00Z\",\n",
        "    \"measurements\": {\n",
        "      \"temperature\": 23.5,\n",
        "      \"humidity\": 56.2\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    \"device_id\": \"sensor2\",\n",
        "    \"timestamp\": \"2024-08-13T10:01:00Z\",\n",
        "    \"measurements\": {\n",
        "      \"temperature\": 22.8\n",
        "    }\n",
        "  }\n",
        "]\n",
        "'''\n",
        "\n",
        "# Convert JSON to DataFrame\n",
        "data = json.loads(json_data)\n",
        "flat_data = [flatten_json(record) for record in data]\n",
        "df = pd.DataFrame(flat_data)\n",
        "\n",
        "# Step 2: Separate Numeric and Non-Numeric Data\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "# Handle Missing Data (Impute only numeric columns)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df_imputed_numeric = pd.DataFrame(imputer.fit_transform(df[numeric_cols]), columns=numeric_cols)\n",
        "\n",
        "# Non-numeric data remains unchanged\n",
        "df_imputed_non_numeric = df[non_numeric_cols]\n",
        "\n",
        "# Concatenate the numeric and non-numeric data\n",
        "df_imputed = pd.concat([df_imputed_numeric, df_imputed_non_numeric], axis=1)\n",
        "\n",
        "# Step 3: Normalize Numeric Data\n",
        "scaler = MinMaxScaler()\n",
        "df_normalized_numeric = pd.DataFrame(scaler.fit_transform(df_imputed_numeric), columns=df_imputed_numeric.columns)\n",
        "\n",
        "# Concatenate normalized numeric data with non-numeric data\n",
        "df_normalized = pd.concat([df_normalized_numeric, df_imputed_non_numeric], axis=1)\n",
        "\n",
        "# Step 4: Build a DNN Model for Schema Transformation\n",
        "def build_dnn_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))  # Example output layer for classification\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Example: Assume the harmonized schema requires a 10-class output\n",
        "input_dim = df_normalized_numeric.shape[1]\n",
        "dnn_model = build_dnn_model(input_dim)\n",
        "\n",
        "# Step 5: Train the DNN Model\n",
        "# Generating synthetic labels for demonstration (Replace with actual labels)\n",
        "labels = np.random.randint(0, 10, size=(df_normalized_numeric.shape[0], 1))\n",
        "labels = tf.keras.utils.to_categorical(labels, num_classes=10)\n",
        "\n",
        "# Training the model\n",
        "dnn_model.fit(df_normalized_numeric, labels, epochs=10, batch_size=2)\n",
        "\n",
        "# Step 6: Inference\n",
        "# Example inference using the model (for the first data point)\n",
        "prediction = dnn_model.predict(df_normalized_numeric.iloc[0:1])\n",
        "print(f\"Predicted schema category: {np.argmax(prediction)}\")\n"
      ]
    }
  ]
}