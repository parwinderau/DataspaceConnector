{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO62YPOfFSahIyYFWHt99d3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parwinderau/DataspaceConnector/blob/main/Simple_JSON_Synthetic_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiyGruV4mS9g",
        "outputId": "44c610b4-372c-4b06-8d3f-2cbe9360d095"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-27.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Downloading Faker-27.0.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m1.4/1.8 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-27.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a25d7HRmNgH",
        "outputId": "768b8366-1d0a-458a-e57d-1629e3644c58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"device_id\": \"baeb342d-d9e8-4833-ae1e-d437c9c133ab\",\n",
            "    \"timestamp\": \"2018-10-25T18:49:49.894426\",\n",
            "    \"measurements\": {\n",
            "      \"temperature\": -0.9168523885050668,\n",
            "      \"humidity\": 15.577727734748246\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"device_id\": \"10cab8a2-9ea2-4c8d-9389-e80d8a892fdc\",\n",
            "    \"timestamp\": \"1977-04-29T16:44:09.032658\",\n",
            "    \"measurements\": {\n",
            "      \"temperature\": 17.40642111889551,\n",
            "      \"humidity\": 57.87882101785955\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"device_id\": \"eeccb19f-cec8-4d5b-996b-213b880a08b7\",\n",
            "    \"timestamp\": \"2017-12-12T04:02:19.651435\",\n",
            "    \"measurements\": {\n",
            "      \"temperature\": 23.046873212378458,\n",
            "      \"humidity\": 17.822500057269934\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"device_id\": \"22d7965a-e07a-4faf-9347-363f9c1e7867\",\n",
            "    \"timestamp\": \"1979-10-10T23:26:13.392379\",\n",
            "    \"measurements\": {\n",
            "      \"temperature\": 22.538488845879005,\n",
            "      \"humidity\": 42.10645209994575\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"sensor_id\": \"d34495a3-5a26-4e51-a871-bfd85d206633\",\n",
            "    \"time\": \"2010-04-28T01:57:31.748625\",\n",
            "    \"readings\": {\n",
            "      \"temp\": 46.60438254407449,\n",
            "      \"humid\": 89.62870669711401\n",
            "    }\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "from faker import Faker\n",
        "from copy import deepcopy\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "# Base template for generating synthetic JSON data\n",
        "base_template = {\n",
        "    \"device_id\": None,\n",
        "    \"timestamp\": None,\n",
        "    \"measurements\": {\n",
        "        \"temperature\": None,\n",
        "        \"humidity\": None\n",
        "    }\n",
        "}\n",
        "\n",
        "# Function to generate synthetic data\n",
        "def generate_data(template, structure_variation=True, semantic_variation=True):\n",
        "    data = deepcopy(template)\n",
        "\n",
        "    # Semantically same information but structurally different\n",
        "    if structure_variation and not semantic_variation:\n",
        "        data = {key: data[key] for key in reversed(list(data.keys()))}  # Reverse the order of keys\n",
        "\n",
        "    # Structurally same but semantically different\n",
        "    elif not structure_variation and semantic_variation:\n",
        "        data[\"device_id\"] = fake.uuid4()\n",
        "        data[\"timestamp\"] = fake.date_time().isoformat()\n",
        "        data[\"measurements\"][\"temperature\"] = random.uniform(-10, 50)  # Different but valid temperature range\n",
        "        data[\"measurements\"][\"humidity\"] = random.uniform(10, 90)  # Different but valid humidity range\n",
        "\n",
        "    # Structural and semantically both different\n",
        "    elif structure_variation and semantic_variation:\n",
        "        data[\"sensor_id\"] = data.pop(\"device_id\")  # Change the key name\n",
        "        data[\"time\"] = data.pop(\"timestamp\")  # Change the key name\n",
        "        data[\"readings\"] = data.pop(\"measurements\")  # Change the key name\n",
        "        data[\"readings\"][\"temp\"] = data[\"readings\"].pop(\"temperature\")  # Change the key name\n",
        "        data[\"readings\"][\"humid\"] = data[\"readings\"].pop(\"humidity\")  # Change the key name\n",
        "        data[\"sensor_id\"] = fake.uuid4()\n",
        "        data[\"time\"] = fake.date_time().isoformat()\n",
        "        data[\"readings\"][\"temp\"] = random.uniform(-10, 50)\n",
        "        data[\"readings\"][\"humid\"] = random.uniform(10, 90)\n",
        "\n",
        "    # Structural and semantically both same\n",
        "    else:\n",
        "        data[\"device_id\"] = fake.uuid4()\n",
        "        data[\"timestamp\"] = fake.date_time().isoformat()\n",
        "        data[\"measurements\"][\"temperature\"] = random.uniform(15, 30)\n",
        "        data[\"measurements\"][\"humidity\"] = random.uniform(30, 70)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Generate a dataset with varying conditions\n",
        "def generate_synthetic_dataset(num_samples=10):\n",
        "    dataset = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        condition = random.choice([\n",
        "            'structure_diff_semantic_same',\n",
        "            'structure_same_semantic_diff',\n",
        "            'structure_semantic_both_diff',\n",
        "            'structure_semantic_both_same'\n",
        "        ])\n",
        "\n",
        "        if condition == 'structure_diff_semantic_same':\n",
        "            data = generate_data(base_template, structure_variation=True, semantic_variation=False)\n",
        "\n",
        "        elif condition == 'structure_same_semantic_diff':\n",
        "            data = generate_data(base_template, structure_variation=False, semantic_variation=True)\n",
        "\n",
        "        elif condition == 'structure_semantic_both_diff':\n",
        "            data = generate_data(base_template, structure_variation=True, semantic_variation=True)\n",
        "\n",
        "        else:  # 'structure_semantic_both_same'\n",
        "            data = generate_data(base_template, structure_variation=False, semantic_variation=False)\n",
        "\n",
        "        dataset.append(data)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Generate and display synthetic data\n",
        "synthetic_data = generate_synthetic_dataset(5)\n",
        "print(json.dumps(synthetic_data, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This is for NLP based JSON data generator for IoT domain\n",
        "import json\n",
        "import random\n",
        "from faker import Faker\n",
        "from copy import deepcopy\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import download\n",
        "\n",
        "# Download necessary NLTK data\n",
        "download('wordnet')\n",
        "download('omw-1.4')\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "# Function to get synonyms from WordNet\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    return list(synonyms)\n",
        "\n",
        "# Generate variations of terms used in JSON keys\n",
        "term_variations = {\n",
        "    \"device_id\": get_synonyms(\"device\")[:5] + [\"sensor_id\", \"equipment_id\", \"machine_id\"],\n",
        "    \"timestamp\": get_synonyms(\"timestamp\")[:5] + [\"time\", \"datetime\", \"log_time\"],\n",
        "    \"measurements\": get_synonyms(\"measurement\")[:5] + [\"readings\", \"data\", \"values\"],\n",
        "    \"temperature\": get_synonyms(\"temperature\")[:5] + [\"temp\", \"heat\", \"thermal\"],\n",
        "    \"humidity\": get_synonyms(\"humidity\")[:5] + [\"moisture\", \"dampness\", \"humid\"],\n",
        "}\n",
        "\n",
        "# Base template for generating synthetic JSON data\n",
        "base_template = {\n",
        "    \"device_id\": None,\n",
        "    \"timestamp\": None,\n",
        "    \"measurements\": {\n",
        "        \"temperature\": None,\n",
        "        \"humidity\": None\n",
        "    }\n",
        "}\n",
        "\n",
        "# Function to generate synthetic data with NLP variations\n",
        "def generate_data(template, structure_variation=True, semantic_variation=True):\n",
        "    data = deepcopy(template)\n",
        "\n",
        "    # Apply term variations\n",
        "    def apply_variations(d):\n",
        "        for key in list(d.keys()):\n",
        "            if isinstance(d[key], dict):\n",
        "                apply_variations(d[key])\n",
        "            new_key = random.choice(term_variations.get(key, [key]))\n",
        "            if new_key != key:\n",
        "                d[new_key] = d.pop(key)\n",
        "\n",
        "    if structure_variation:\n",
        "        apply_variations(data)\n",
        "\n",
        "    # Semantically same but structurally different\n",
        "    if structure_variation and not semantic_variation:\n",
        "        pass  # Already handled by apply_variations\n",
        "\n",
        "    # Structurally same but semantically different\n",
        "    elif not structure_variation and semantic_variation:\n",
        "        data[\"device_id\"] = fake.uuid4()\n",
        "        data[\"timestamp\"] = fake.date_time().isoformat()\n",
        "        data[\"measurements\"][\"temperature\"] = random.uniform(-10, 50)\n",
        "        data[\"measurements\"][\"humidity\"] = random.uniform(10, 90)\n",
        "\n",
        "    # Structural and semantically both different\n",
        "    elif structure_variation and semantic_variation:\n",
        "        data[\"device_id\"] = fake.uuid4()\n",
        "        data[\"timestamp\"] = fake.date_time().isoformat()\n",
        "        data[\"measurements\"][\"temperature\"] = random.uniform(-10, 50)\n",
        "        data[\"measurements\"][\"humidity\"] = random.uniform(10, 90)\n",
        "\n",
        "    # Structural and semantically both same\n",
        "    else:\n",
        "        data[\"device_id\"] = fake.uuid4()\n",
        "        data[\"timestamp\"] = fake.date_time().isoformat()\n",
        "        data[\"measurements\"][\"temperature\"] = random.uniform(15, 30)\n",
        "        data[\"measurements\"][\"humidity\"] = random.uniform(30, 70)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Generate a dataset with varying conditions\n",
        "def generate_synthetic_dataset(num_samples=10):\n",
        "    dataset = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        condition = random.choice([\n",
        "            'structure_diff_semantic_same',\n",
        "            'structure_same_semantic_diff',\n",
        "            'structure_semantic_both_diff',\n",
        "            'structure_semantic_both_same'\n",
        "        ])\n",
        "\n",
        "        if condition == 'structure_diff_semantic_same':\n",
        "            data = generate_data(base_template, structure_variation=True, semantic_variation=False)\n",
        "\n",
        "        elif condition == 'structure_same_semantic_diff':\n",
        "            data = generate_data(base_template, structure_variation=False, semantic_variation=True)\n",
        "\n",
        "        elif condition == 'structure_semantic_both_diff':\n",
        "            data = generate_data(base_template, structure_variation=True, semantic_variation=True)\n",
        "\n",
        "        else:  # 'structure_semantic_both_same'\n",
        "            data = generate_data(base_template, structure_variation=False, semantic_variation=False)\n",
        "\n",
        "        dataset.append(data)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Generate and display synthetic data\n",
        "synthetic_data = generate_synthetic_dataset(5)\n",
        "print(json.dumps(synthetic_data, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSKf7qLknbfp",
        "outputId": "c09b36ff-1b13-4d40-8839-972f6447a96c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"device_id\": \"f70f6a2f-9afb-4e4e-b062-2466476d0105\",\n",
            "    \"timestamp\": \"1995-05-12T08:43:15.351342\",\n",
            "    \"measurements\": {\n",
            "      \"temperature\": 21.205621152364852,\n",
            "      \"humidity\": 17.171543068727644\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"device\": null,\n",
            "    \"time\": null,\n",
            "    \"mensuration\": {\n",
            "      \"humidity\": null,\n",
            "      \"temp\": null\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"device_id\": \"20cccd6f-5904-4373-8b59-ebc5abc0ec9d\",\n",
            "    \"timestamp\": \"2002-10-23T02:30:10.833039\",\n",
            "    \"measurements\": {\n",
            "      \"temperature\": 24.250107522842594,\n",
            "      \"humidity\": 41.437023672510215\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"twist\": null,\n",
            "    \"time\": null,\n",
            "    \"measure\": {\n",
            "      \"temperature\": null,\n",
            "      \"humidity\": null\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"sensor_id\": null,\n",
            "    \"datetime\": null,\n",
            "    \"measuring\": {\n",
            "      \"temperature\": null,\n",
            "      \"humidness\": null\n",
            "    }\n",
            "  }\n",
            "]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from faker import Faker\n",
        "from copy import deepcopy\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import download\n",
        "\n",
        "# Download necessary NLTK data\n",
        "download('wordnet')\n",
        "download('omw-1.4')\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "# Function to get synonyms from WordNet\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    return list(synonyms)\n",
        "\n",
        "# Generate variations of terms used in JSON keys\n",
        "term_variations = {\n",
        "    \"device_id\": get_synonyms(\"device\")[:5] + [\"sensor_id\", \"equipment_id\", \"machine_id\"],\n",
        "    \"timestamp\": get_synonyms(\"timestamp\")[:5] + [\"time\", \"datetime\", \"log_time\"],\n",
        "    \"measurements\": get_synonyms(\"measurement\")[:5] + [\"readings\", \"data\", \"values\"],\n",
        "    \"temperature\": get_synonyms(\"temperature\")[:5] + [\"temp\", \"heat\", \"thermal\"],\n",
        "    \"humidity\": get_synonyms(\"humidity\")[:5] + [\"moisture\", \"dampness\", \"humid\"],\n",
        "}\n",
        "\n",
        "# Base template for generating synthetic JSON data\n",
        "base_template = {\n",
        "    \"device_id\": None,\n",
        "    \"timestamp\": None,\n",
        "    \"measurements\": {\n",
        "        \"temperature\": None,\n",
        "        \"humidity\": None\n",
        "    }\n",
        "}\n",
        "\n",
        "# Function to generate synthetic data with NLP variations\n",
        "def generate_data(template, structural_variation=True, semantic_variation=True):\n",
        "    data = deepcopy(template)\n",
        "\n",
        "    # Apply term variations (semantic variation)\n",
        "    def apply_semantic_variations(d):\n",
        "        for key in list(d.keys()):\n",
        "            if isinstance(d[key], dict):\n",
        "                apply_semantic_variations(d[key])\n",
        "            new_key = random.choice(term_variations.get(key, [key]))\n",
        "            if new_key != key:\n",
        "                d[new_key] = d.pop(key)\n",
        "\n",
        "    if semantic_variation:\n",
        "        apply_semantic_variations(data)\n",
        "\n",
        "    # Apply structural variation\n",
        "    if structural_variation:\n",
        "        if random.choice([True, False]):  # Randomly decide to flatten\n",
        "            flattened_data = {}\n",
        "            for key, value in data.items():\n",
        "                if isinstance(value, dict):\n",
        "                    for subkey, subvalue in value.items():\n",
        "                        flattened_data[f\"{key}_{subkey}\"] = subvalue\n",
        "                else:\n",
        "                    flattened_data[key] = value\n",
        "            data = flattened_data\n",
        "\n",
        "    # Generate actual data values if not already present\n",
        "    if not semantic_variation:\n",
        "        data[\"device_id\"] = fake.uuid4()\n",
        "        data[\"timestamp\"] = fake.date_time().isoformat()\n",
        "        data[\"measurements\"][\"temperature\"] = random.uniform(-10, 50)\n",
        "        data[\"measurements\"][\"humidity\"] = random.uniform(10, 90)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Generate a dataset with varying conditions\n",
        "def generate_synthetic_dataset(num_samples=10):\n",
        "    dataset = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        data = generate_data(base_template,\n",
        "                            structural_variation=random.choice([True, False]),\n",
        "                            semantic_variation=random.choice([True, False]))\n",
        "        dataset.append(data)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Generate and display synthetic data\n",
        "synthetic_data = generate_synthetic_dataset(5)\n",
        "print(json.dumps(synthetic_data, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP469MoyrYEl",
        "outputId": "542ec403-ff87-4750-d28d-bb829435fffa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"device_id\": \"4b8a65b8-7535-4bbd-84c1-0849d9350efc\",\n",
            "    \"timestamp\": \"1995-02-16T03:43:46.261065\",\n",
            "    \"measurements\": {\n",
            "      \"temperature\": 34.061100695140134,\n",
            "      \"humidity\": 76.96917312512983\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"gimmick\": null,\n",
            "    \"datetime\": null,\n",
            "    \"readings\": {\n",
            "      \"temp\": null,\n",
            "      \"humidness\": null\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"device_id\": \"a2e39263-f663-42a0-8168-6a668ae9c0b5\",\n",
            "    \"timestamp\": \"2003-12-27T17:14:32.167831\",\n",
            "    \"measurements\": {\n",
            "      \"temperature\": -0.8889444020204778,\n",
            "      \"humidity\": 21.915546841070856\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"equipment_id\": null,\n",
            "    \"log_time\": null,\n",
            "    \"data\": {\n",
            "      \"temperature\": null,\n",
            "      \"dampness\": null\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"device_id\": \"cd0e059d-38bf-4172-8ac0-1efbf9584e9e\",\n",
            "    \"timestamp\": \"2016-01-19T00:42:36.902679\",\n",
            "    \"measurements\": {\n",
            "      \"temperature\": 36.64447775006671,\n",
            "      \"humidity\": 33.43464361849385\n",
            "    }\n",
            "  }\n",
            "]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import download\n",
        "from transformers import pipeline\n",
        "\n",
        "# Download necessary NLTK data\n",
        "download('wordnet')\n",
        "download('omw-1.4')\n",
        "\n",
        "# NLP pipeline for entity and attribute generation\n",
        "nlp = pipeline(\"text-generation\")\n",
        "\n",
        "def generate_machine_data(machine_type, depth, size):\n",
        "    # Generate core entities and attributes using NLP\n",
        "    machine_description = f\"A {machine_type} is a machine that\"\n",
        "    machine_attributes = nlp(machine_description, max_length=100, num_return_sequences=5)\n",
        "\n",
        "    # Extract key entities and attributes\n",
        "    entities = []\n",
        "    for text_data in machine_attributes:\n",
        "        entities.extend(text_data['generated_text'].split(','))\n",
        "    entities = list(set(entities))\n",
        "\n",
        "    # Create JSON structure\n",
        "    data = {}\n",
        "    for _ in range(random.randint(2, size)):\n",
        "        root_key = random.choice(entities)\n",
        "        data[root_key] = {}\n",
        "        for _ in range(depth - 1):\n",
        "            sub_key = random.choice(entities)\n",
        "            data[root_key][sub_key] = {}\n",
        "        # Populate leaf nodes with random data\n",
        "        data[root_key][sub_key] = {\n",
        "            'value': random.uniform(0, 100)\n",
        "        }\n",
        "\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "machine_types = ['boiler', 'compressor', ...]  # Full list of machine types\n",
        "dataset = []\n",
        "for machine_type in machine_types:\n",
        "    dataset.append(generate_machine_data(machine_type, 3, 5))\n",
        "\n",
        "print(json.dumps(dataset, indent=2))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgfnko19tB0B",
        "outputId": "bc003dc4-7902-4281-b684-377766a11ec6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \" combined with additional hydrogen added outside of the boiler\": {\n",
            "      \" does not require specialized tools. The heating and cooling system is a simple electrical circuit\": {},\n",
            "      \" because of its size\": {\n",
            "        \"value\": 44.652330094907455\n",
            "      }\n",
            "    },\n",
            "    \"A boiler is a machine that has been subjected to continuous firing by the heat of the steam or steam engine for several hours. A steam boiler is a type of boiler that\": {\n",
            "      \" boilers can actually provide the power needed when the stove is run. When it goes silent the boiler creates steam when it's turned off. There is an average of 2A/1W at the lower boilers\": {},\n",
            "      \"A boiler is a machine that uses electricity to process a mixture of hydrogen (OH) and helium in the air\\u2014an important component of a heat exchanger\": {\n",
            "        \"value\": 36.19726505126067\n",
            "      }\n",
            "    },\n",
            "    \" any place you want or want not to have. We have a lot of boiler rooms in Australia now and they're just huge. You could build a house up here in Perth\": {\n",
            "      \"A boiler is a machine that has been subjected to continuous firing by the heat of the steam or steam engine for several hours. A steam boiler is a type of boiler that\": {},\n",
            "      \" because of its size\": {\n",
            "        \"value\": 9.047613816124212\n",
            "      }\n",
            "    },\n",
            "    \" with one for the boiler and several boiler parts. (Actually\": {\n",
            "      \"A boiler is a machine that burns electricity.\\n\\nA boiler is a machine that burns electricity. Air pumps are devices that heat your home directly off the grid. They heat up air currents. They do a lot more than just heat water. They allow us electric power. But you need a water heater. Water pumps create a lot more electricity. The water pump is connected in a series and has the most efficient and efficient method of moving electricity out of heat. A home with a water heating plant\": {},\n",
            "      \" steam and other liquid on the floor during its own time. It is in fact the only type of boiler available in the U.S.\": {\n",
            "        \"value\": 16.076482479016974\n",
            "      }\n",
            "    },\n",
            "    \" so this is a type that\": {\n",
            "      \" will heat the water\": {},\n",
            "      \" with one for the boiler and several boiler parts. (Actually\": {\n",
            "        \"value\": 79.29962756681252\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \" and the speed at which they keep one level of noise steady\": {\n",
            "      \" if these machines were designed with a\": {},\n",
            "      \" is very simple and very predictable. So\": {\n",
            "        \"value\": 34.35973397240255\n",
            "      }\n",
            "    },\n",
            "    \" there's no need for anything other than their name.\\n\\nWhy isn't there an alternative to the old method of calculating per octave? Why aren't there all other non-equimetric methods? Why don't we see any of that? Why are there only some other (usually more complex) methods of measuring how well a compressor is\": {\n",
            "      \" each compressor has different frequency response\": {},\n",
            "      \" air pressure for controlling the compressor compressor and a set of six other compressors. These\": {\n",
            "        \"value\": 1.5215186571711836\n",
            "      }\n",
            "    },\n",
            "    \" water from a compressor. This means it uses more power because it has more power means it can run faster - more power is usually generated since the compressor can run at a lower speed.\\n\\n\\nThe compressor also features two separate compressors as well as a two door compressor - one for controlling the volume\": {\n",
            "      \" each compressor has different frequency response\": {},\n",
            "      \" if these machines were designed with a\": {\n",
            "        \"value\": 23.469841336375442\n",
            "      }\n",
            "    },\n",
            "    \" air pressure for controlling the compressor compressor and a set of six other compressors. These\": {\n",
            "      \" and several different types of noise resistance.\\n\\nEach compressor works best if it produces the sound level that's expected. A simple compressor with a typical frequency response that's high and low can provide the perfect sound.\": {},\n",
            "      \" ranging from electric sources\": {\n",
            "        \"value\": 97.13763850461098\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \" non-linear calculation makes Ellipsis incredibly useful for the production of scientific research.\\n\\nIn this article\": {\n",
            "      \" like CELT.\\n\\n15GB: Used when running 3G or 4G apps\": {},\n",
            "      \"A Ellipsis is a machine that produces waves that are then used to compress the atoms that form the wave. In recent years\": {\n",
            "        \"value\": 51.6634798472403\n",
            "      }\n",
            "    },\n",
            "    \" I will write about Ellipsis in five ways and then show why the machine at the top of the screen might be beneficial.\\n\": {\n",
            "      \"A Ellipsis is a machine that allows the user to select from 3 different forms which allows more interaction than the traditional desktop computer. It contains different features. Here's a diagram\": {},\n",
            "      \" an article appeared in Science describing the possibility of transforming an electron to a gas through electrostatic charge\": {\n",
            "        \"value\": 27.64165738339749\n",
            "      }\n",
            "    },\n",
            "    \" and information from two different sources \\u2013 into one single\": {\n",
            "      \"A Ellipsis is a machine that combines several types of information \\u2013 including data\": {},\n",
            "      \" efficient and efficient machine. A Ellipsis uses two different processor to perform this process.\\n\\nThe Ellipsis supports the following processor types\": {\n",
            "        \"value\": 52.39278129823678\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import download\n",
        "from transformers import pipeline\n",
        "\n",
        "# Download necessary NLTK data\n",
        "download('wordnet')\n",
        "download('omw-1.4')\n",
        "\n",
        "# NLP pipeline for entity and attribute generation\n",
        "nlp = pipeline(\"text-generation\")\n",
        "\n",
        "def shorten_key(key, max_len=3):\n",
        "  return key[:max_len]\n",
        "\n",
        "def generate_machine_data(machine_type, depth, size):\n",
        "  # Generate core entities and attributes using NLP\n",
        "  machine_description = f\"A {machine_type} is a machine that\"\n",
        "  machine_attributes = nlp(machine_description, max_length=100, num_return_sequences=5)\n",
        "\n",
        "  # Extract key entities and attributes\n",
        "  entities = []\n",
        "  for text_data in machine_attributes:\n",
        "    entities.extend(text_data['generated_text'].split(','))\n",
        "  entities = list(set(entities))\n",
        "\n",
        "  # Create JSON structure\n",
        "  data = {}\n",
        "  for _ in range(random.randint(2, size)):\n",
        "    root_key = shorten_key(random.choice(entities))\n",
        "    data[root_key] = {}\n",
        "    for _ in range(depth - 1):\n",
        "      sub_key = shorten_key(random.choice(entities))\n",
        "      data[root_key][sub_key] = {}\n",
        "    # Populate leaf nodes with random data\n",
        "    data[root_key][sub_key] = {\n",
        "      'value': random.uniform(0, 100)\n",
        "    }\n",
        "\n",
        "  return data\n",
        "\n",
        "# Example usage\n",
        "machine_types = ['boiler', 'compressor', ...]\n",
        "# Full list of machine types\n",
        "dataset = []\n",
        "for machine_type in machine_types:\n",
        "  dataset.append(generate_machine_data(machine_type, 3, 5))\n",
        "\n",
        "print(json.dumps(dataset, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbjcOyFa2NPn",
        "outputId": "70da8214-bd76-4b80-98d3-a1a81cca68cd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \" or\": {\n",
            "      \" re\": {},\n",
            "      \" ra\": {\n",
            "        \"value\": 29.347744512703567\n",
            "      }\n",
            "    },\n",
            "    \" ho\": {\n",
            "      \" an\": {},\n",
            "      \" te\": {\n",
            "        \"value\": 34.67956618987742\n",
            "      }\n",
            "    },\n",
            "    \" an\": {\n",
            "      \" te\": {},\n",
            "      \" an\": {\n",
            "        \"value\": 85.54038428135928\n",
            "      }\n",
            "    },\n",
            "    \"A b\": {\n",
            "      \" an\": {},\n",
            "      \" ra\": {\n",
            "        \"value\": 69.92615774716863\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \" th\": {\n",
            "      \"A c\": {},\n",
            "      \" bu\": {\n",
            "        \"value\": 54.17633615026849\n",
            "      }\n",
            "    },\n",
            "    \" ad\": {\n",
            "      \" ad\": {},\n",
            "      \"A c\": {\n",
            "        \"value\": 53.65096891085324\n",
            "      }\n",
            "    },\n",
            "    \" bu\": {\n",
            "      \"A c\": {\n",
            "        \"value\": 55.60562708929512\n",
            "      }\n",
            "    },\n",
            "    \"A c\": {\n",
            "      \" a \": {},\n",
            "      \" th\": {\n",
            "        \"value\": 86.93883470567309\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"A E\": {\n",
            "      \" as\": {},\n",
            "      \" or\": {\n",
            "        \"value\": 44.3266823352529\n",
            "      }\n",
            "    },\n",
            "    \" wi\": {\n",
            "      \" wi\": {},\n",
            "      \"A E\": {\n",
            "        \"value\": 91.38661968673505\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import download\n",
        "from transformers import pipeline, AutoModel, AutoTokenizer\n",
        "import spacy\n",
        "\n",
        "# Download necessary NLTK data\n",
        "download('stopwords')\n",
        "download('punkt')\n",
        "\n",
        "# NLP pipelines\n",
        "ner_model = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")  # Replace with your preferred NER model\n",
        "model_name = \"bert-base-uncased\"  # Replace with your preferred BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Load the NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Replace with your preferred model\n",
        "\n",
        "def extract_keywords(text):\n",
        "    # NER for potential keywords\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    keywords = [entity[0] for entity in entities]\n",
        "\n",
        "    # BERT embeddings for keyword ranking (optional)\n",
        "    # ...\n",
        "\n",
        "    return keywords\n",
        "\n",
        "def generate_machine_data(machine_type, depth, size):\n",
        "    # Generate core entities and attributes using NLP\n",
        "    machine_description = f\"A {machine_type} is a machine that\"\n",
        "    machine_attributes = nlp(machine_description)\n",
        "\n",
        "    # Create JSON structure\n",
        "    data = {}\n",
        "    for text_data in machine_attributes:\n",
        "        keywords = extract_keywords(text_data.text)\n",
        "        for keyword in keywords:\n",
        "            if keyword not in data:\n",
        "                data[keyword] = {}\n",
        "            # Build nested structure based on subsequent keywords and values\n",
        "            # ...\n",
        "\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "machine_types = ['boiler', 'compressor', ...]\n",
        "# Full list of machine types\n",
        "dataset = []\n",
        "for machine_type in machine_types:\n",
        "    dataset.append(generate_machine_data(machine_type, 3, 5))\n",
        "\n",
        "print(json.dumps(dataset, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnYDpxv06PEg",
        "outputId": "e0f88a17-b85e-48b9-800d-b4a305a03690"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {},\n",
            "  {},\n",
            "  {}\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import download\n",
        "from transformers import pipeline, AutoModel, AutoTokenizer\n",
        "import spacy\n",
        "\n",
        "# Download necessary NLTK data\n",
        "download('stopwords')\n",
        "download('punkt')\n",
        "\n",
        "# NLP pipelines\n",
        "ner_model = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")  # Replace with your preferred NER model\n",
        "model_name = \"bert-base-uncased\"  # Replace with your preferred BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Load the NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Replace with your preferred model\n",
        "\n",
        "def extract_keywords(text):\n",
        "    # NER for potential keywords\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    keywords = [entity[0] for entity in entities]\n",
        "\n",
        "    # Print keywords for debugging\n",
        "    print(keywords)  # Check if keywords are extracted correctly\n",
        "\n",
        "    return keywords\n",
        "\n",
        "def generate_machine_data(machine_type, depth, size):\n",
        "    # Generate core entities and attributes using NLP\n",
        "    machine_description = f\"A {machine_type} is a machine that\"\n",
        "    machine_attributes = nlp(machine_description)\n",
        "\n",
        "    # Create JSON structure\n",
        "    data = {}\n",
        "    for text_data in machine_attributes:\n",
        "        keywords = extract_keywords(text_data.text)\n",
        "        for keyword in keywords:\n",
        "            if keyword not in data:\n",
        "                data[keyword] = {}\n",
        "            # Build nested structure based on subsequent keywords and values\n",
        "            # ...\n",
        "\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "machine_types = ['boiler', 'compressor', ...]\n",
        "# Full list of machine types\n",
        "dataset = []\n",
        "for machine_type in machine_types:\n",
        "    dataset.append(generate_machine_data(machine_type, 3, 5))\n",
        "\n",
        "print(json.dumps(dataset, indent=2))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efHG0CmG8LWO",
        "outputId": "d4e7bcfd-bdf2-43ec-fa5f-a6a627ae3f63"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[\n",
            "  {},\n",
            "  {},\n",
            "  {}\n",
            "]\n"
          ]
        }
      ]
    }
  ]
}